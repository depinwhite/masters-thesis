\subsection{Обучающая система и экспериментальные настройки}

В этом разделе описаны методология и возникшие в процессе построения системы трудности. Для оценки качества предлагаемого подхода использована Sequence Tagging модель GECToR, показавшая высокую эффективность в задаче исправления грамматических ошибок. Для корректного сравнения с авторами модели использованы те же данные и повторен процесс обучения с теми же параметрами.

\begin{table}[h]
  \centering
  \small 
  \begin{tabular}{c*{2}{c}c}
    \hline
    \multirow{1}{*}{\textbf{\shortstack{Датасет}}} & \multicolumn{2}{c}{\textbf{Количество предложений}} & \multirow{1}{*}{\textbf{Этап}}  \\
     & Уровень подслов  & Уровень слов & \textbf{обучения} \\
    \hline
     PIE-synthetic &  9,000,000 & 9,000,000 & I \\
     Lang-8 & 787,613 & 947,344 & II  \\
     NUCLE & 51,929 & 56,958 & II  \\
     FCE & 25,968 & 34,490 & II \\
     W\&I+LOCNESS & 21,828 & 34,304 & II, III \\
    \hline
  \end{tabular}
 \caption{Наборы данных на каждом этапе обуччения с соответствующим количеством предложений: в исследовании~\cite{b15} на уровне слов и в нашем исследовании на уровне подслов.}
  \label{tab:data}
\end{table}

\subsection{Описание данных}

В таблице~\ref{tab:data} представлены статистики данных, использованных в исследовании~\cite{b15} и в нашем исследовании. Было использовано пять датасетов: PIE-synthetic~\cite{b32}, Lang-8~\cite{b33, b34}, NUCLE~\cite{b35}, FCE~\cite{b36} и W\&I+LOCNESS~\cite{b37}.

\subsubsection{Ограничения алгоритма}

Слабым местом данного подхода является то, что обратный проход использует рекурсивные вызовы для нахождения всевозможных редакционных предписаний и выбора наилучшего. Таким образом, для длинных последовательностей с большим количеством различий возникает множество путей. Для некоторых предложений не удалось найти эталонную последовательность корректирующих преобразований, и они были исключены из обучающего набора.

\subsubsection{Обучающие данные}

Как показано в таблице \ref{tab:data}, на первом этапе обучения, аналогично~\cite{b15}, использовались 9 миллионов предложений из PIE-synthetic. Однако ввиду ограничений только 6.7M предложений совпадают с теми, что использовались в~\cite{b15}. Чтобы довести общий объём до 9M, использовались оставшиеся предложения из PIE-synthetic.

Что касается реальных данных, из таблицы \ref{tab:data} видно, что использовалось только 78\% от общего объёма. Эти данные применялись для дообучения на этапах II и III.

\subsubsection{Данные для оценки}

Для сравнения качества работы моделей использовались те же тестовые наборы, что и в~\cite{b15} --- CoNLL-2014 и BEA-2019. В процессе оценки применялись метрики $M^2$ и ERRANT соответственно.

\begin{table}[h]
  \centering
  \small 
  \begin{tabular}{c*{2}{c}cc}
    \hline
    \multirow{2}{*}{\textbf{\shortstack{Этап \\ обучения}}} & \multicolumn{2}{c}{\textbf{Уровень слов}} & \multicolumn{2}{c}{\textbf{Уровень подслов}} \\
     & Количество эпох & Размер батча & Количество эпох & Размер батча \\
    \hline
     I & 20 & 256 & 20 & 32 \\
     II & 9 & 128 & 7 & 16 \\
     III & 4 & 128 & 1 & 16 \\
    \hline
  \end{tabular}
 \caption{Лучшие эпохи на каждом этапе обучения модели XLNet в~\cite{b15} на уровне слов и в нашем исследовании на уровне подслов с соответствующими размерами батча.}
  \label{tab:epochs}
\end{table}

% \begin{table*}[h]
%   \centering
%   \begin{tabular}{lcccccc}
%     \hline
%     \multirow{2}{*}{Модель} & \multicolumn{3}{c}{\textbf{CoNLL-2014 (тест)}} & \multicolumn{3}{c}{\textbf{BEA-2019 (тест)}} \\
%      & P & R & $F_{0.5}$ & P & R & $F_{0.5}$ \\
%     \hline
%     GECToR (подсловный уровень + XLNet) & 72.3 & 40.4 & 62.4 & 70.5 & 41.6 & 61.9 \\
%     \hline
%     GECToR (словесный уровень + BERT) & 72.1 & \textbf{42.0} & 63.0 & 71.5 & \textbf{55.7} & 67.6 \\ 
%     GECToR (словесный уровень + RoBERTa) & 73.9 & 41.5 & 64.0 & 77.2 & 55.1 & 71.5 \\
%     GECToR (словесный уровень + XLNet) & \textbf{77.5} & 40.1 & \textbf{65.3} & \textbf{79.2} & 53.9 & \textbf{72.4} \\
%     \hline
%   \end{tabular}
%   \caption{Сравнение моделей на подсловном и словесном уровнях. Приведены $M^2$ для CoNLL-2014 и ERRANT для BEA-2019.}
%   \label{tab:result}
% \end{table*}

\begin{table*}
  \centering
  \begin{tabular}{lcccccc}
    \hline
    \multirow{2}{*}{Model} & \multicolumn{3}{c}{\textbf{CoNLL-2014}} & \multicolumn{3}{c}{\textbf{BEA-2019}} \\
     & P & R & $F_{0.5}$ & P & R & ${F}_{0.5}$ \\
    \hline
    GECToR & 72.3 & 40.4 & 62.4 & 70.5 & 41.6 & 61.9 \\
    (уровень подслов + XLNet) & & & & & & \\
    \hline
    GECToR & 72.1 & \textbf{42.0} & 63.0 & 71.5 & \textbf{55.7} & 67.6 \\ 
    (уровень слов + BERT) & & & & & & \\
    GECToR & 73.9 & 41.5 & 64.0 & 77.2 & 55.1 & 71.5 \\
    (уровень слов + RoBERTa) & & & & & & \\
    GECToR & \textbf{77.5} & 40.1 & \textbf{65.3} & \textbf{79.2} & 53.9 & \textbf{72.4} \\
    (уровень слов + XLNet) & & & & & & \\
    % GECToR & 78.2 & 41.5 & 66.5 & 78.9 & 58.2 & 73.6 \\
    % (word-level + BERT + RoBERTa + XLNet) & & & & & & \\
    \hline
  \end{tabular}
  \caption{Сравнение работы моделей на уровне слов и на уровне подслов. Приведены оценки $M^2$ для CoNLL-2014 (тест) и ERRANT для BEA-2019 (тест).}
  \label{tab:result}
\end{table*}


\subsection{Обучение модели}

Архитектура модели GECToR состоит из кодировщика на базе архитектуры “Трансформер” и двух линейных классификаторов. Первый классификатор предсказывает наличие ошибки в каждом подслове. Второй классификатор выбирает конкретное правило из словаря для исправления подслова или оставляет его без изменений, если наиболее вероятное правило --- \textit{keep}.

Если максимальная вероятность первого классификатора по предложению ниже определённого порога, предложение не подлежит исправлению. В~\cite{b15} этот порог равен 0.66. В нашем исследовании эмпирическим путем было подобрано оптимальное значение 0.05.

\subsubsection{Исправление на уровне слов}

В~\cite{b15} словарь правил исправления был построен на уровне слов. Это означает, что несколько подслов в одном слове соотносятся с одним правилом из словаря.

В исследовании использовались два типа правил: “базовые преобразования” и “g-преобразования”. Базовые преобразования — это \textit{delete}, \textit{keep}, \textit{replace} и \textit{append}. G-преобразования --- это более сложные грамматические изменения: смена формы глагола, слияние/разде-ление слов, изменение числа существительного и др. Для построения g-трансформаций необходимо знание грамматики языка, аннотированные данные и средства сопоставления предсказанных правил с эталонными.

Весь словарь включает 5000 правил, из которых 4971 --- базовые преобразования, 29 --- g-преобразования.

\subsubsection{Исправление на уровне подслов}

В нашем подходе отсутствуют g-преобразования, что избавляет нас от необходимости вручную составлять правила. Это особенно важно для языков с богатой морфологией, для которых обобщение грамматики вручную крайне трудоёмко.

Нами был получен словарь правил на уровне подслов, используя только базовые преобразования: \textit{delete}, \textit{keep}, \textit{replace} и \textit{append}. Процесс полностью автоматический и не требует аннотированных данных, лишь параллельный корпус. Объём итогового словаря составил 25,714 правил.

\subsubsection{Параметры обучения}

В качестве кодировщика использован XLNet, так как он показал наилучшие результаты в~\cite{b15}. Процесс обучения был повторён с теми же параметрами, за исключением размера батча.

Авторы~\cite{b15} использовали размер батча 256 на первом этапе обучения и 128 на втором и третьем этапах. В нашем случае значения составили 32 и 16 соответственно. В таблице~\ref{tab:epochs} представлены лучшие эпохи для каждого этапа.

