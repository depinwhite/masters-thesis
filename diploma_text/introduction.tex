\newpage


\section{Введение}
% В современном мире проблема автоматического исправления грамматических ошибок (ИГО) является актуальной~\cite{b1, b2, b3} в связи с увеличением объема текстовых данных~\cite{b4}. Ручная проверка больших текстов является трудоемкой задачей. Кроме того, решение данной задачи имеет специфические приложения: проверка сочинений~\cite{b5}; исправление сообщений в социальных сетях~\cite{b6}; исправление текстов, написанных иностранными студентами~\cite{b7}; поиск текстовых заимствований~\cite{b8}. Последнее объясняется тем, что грамматические ошибки являются одним из способов обхода системы поиска текстовых заимствований~\cite{b9}. Для обучения моделей поиска текстовых заимствований используются тексты научных статей, содержащие небольшое количество ошибок. Пользователи при этом могут нарочно допускать множество ошибок, влияя тем самым на предсказания модели. Это приводит к появлению состязательных атак (англ. adversarial attacks) на модели обработки естественного языка~\cite{b10}. Большинство исследований в области ИГО ориентировано на английский язык~\cite{b11}. Для многих других языков, включая русский, число исследований значительно меньше~\cite{b12}. Особенно стоит отметить сложную морфологию русского языка, что усложняет решение задачи ИГО.
% В настоящее время существует два наиболее эффективных подхода решения задачи ИГО для английского языка, – Sequence-to-Sequence (Seq2Seq)~\cite{b13, b14} и Sequence Tagging (ST)~\cite{b15, b16}. В подходе Seq2Seq исходная последовательность представляет собой предложение с ошибками, а целевая последовательность – предложение без ошибок. Такой подход решения задачи ИГО работает хорошо, но обладают низкой скоростью работы, а также плохой интерпретируемостью, поскольку для определения типа ошибки необходимо использовать дополнительный функционал. Модели, основанные на подходе ST, не имеют данных проблем: не требуется полностью генерировать предложение без ошибок, достаточно лишь отметить ошибки в исходном предложении. Кроме того, они легко интерпретируемы, так как решают задачу классификации для каждого токена, сопоставляя ему нужное правило из заданного словаря корректирующих правил. Таким образом, для объяснения исправления, применяемого к токену, не требуется дополнительный функционал.
% В~\cite{b12} авторы сравнивают данные подходы для решения задачи ИГО в текстах на русском языке. Результаты показывают, что на небольшом объеме размеченных данных методы, основанные на подходе ST, существенно превосходят методы, основанные на подходе Seq2Seq. Для сравнения методов авторы представляют набор данных RULEC, содержащий предложения с размеченными грамматическими ошибками. Набор данных состоит из сочинений иностранных студентов.
% Для решения задачи ИГО на английской языке наиболее эффективной ST-моделью является GECToR~\cite{b15}. Данная модель состоит из кодировщика на основе архитектуры трансформер~\cite{b17} и двухголового классификатора. Авторы статьи провели сравнение использования моделей BERT~\cite{b18}, RoBERTa~\cite{b19} и XLNet~\cite{b20} в качестве кодировщиков. Первая голова предсказывает наличие ошибки, а вторая – соответствующее правило для исправления ошибки. После этого соответствующее правило применяется к каждому токену последовательности для исправления. Если токен не содержит ошибок, то данному токену ставится в соответствие правило “\$KEEP”, которое оставляет данный токен без изменений. Обучение модели GECToR состоит из трех этапов: 
% \begin{enumerate}
%     \item обучение на синтетических данных;
%     \item дообучение на корпусе данных, содержащем ошибки;
%     \item дообучение на комбинации из корпусов данных с ошибками и без ошибок.  
% \end{enumerate}
% В рамках магистерской диссертации были проделаны два основных исследования:
% \begin{enumerate}
%     \item в работе предложен алгоритм генерации синтетического набора
% данных, содержащего тексты на русском языке для первого этапа обучения. Модель GECToR адаптирована в модель RuGECToR~\cite{b21} для ИГО в русскоязычных текстах с использованием сгенерированных синтетических данных и данных из открытых источников. Данная модель не требует большого количества данных для обучения, в отличие от моделей на основе архитектуры Seq2Seq, и показывает конкурентноспособное качество для английского языка. Целью данного исследования является обучение модели, способной обобщать морфологические свойства русского языка, а не подстраиваться под обучающую выборку;
%     \item в рамках второго исследовании мы предложили методику автоматического построения словаря корректирующих правил и обучения с помощью него ST-моделей. Наш подход основан на переходе к уровню SentencePiece-токенов~\cite{b22} в GEC. Мы использовали только “базовые преобразования”, общие для всех языков – \textit{keep}, \textit{append}, \textit{replace} и \textit{delete}.  Таким образом, мы полностью избавились от грамматических зависимостей. Для экспериментов мы применили наш подход к ST-модели GECToR. Мы показали, что такой метод дает сопоставимые по качеству результаты, при этом является полностью автоматическим. Кроме того, этот подход можно адаптировать к любому языку, поскольку он не требует знания грамматических правил или ручных аннотаций.
% \end{enumerate}

В настоящее время задача автоматической коррекции грамматических ошибок приобретает всё большую значимость~\cite{b1, b2, b3}, что связано с ростом объёмов текстовой информации~\cite{b4}. Проверка больших текстов вручную требует значительных затрат времени и усилий. Таким образом, исследования в данной области необходимы для автоматизации данных процеессов. Например, при проверке письменных работ~\cite{b5} преподаватели тратят большое количество времени на оценку каждого ученика, хотя было бы менее трудоемко для первичной оценки использовать автоматический алгоритм, а далее валидировать полученный результат с помощью учителя. Сюда же относится и проверка грамотности в социальных сетях --- хотя автоматическая проверка орфографии уже существует, исправление грамматики до сих пор остаётся нерешённой задачей~\cite{b6}. Это может быть использовано не только для социальных сетей, но и для разработки приложения по изучению языков --- редактирование текстов, созданных иностранными студентами~\cite{b7} Также намеренные грамматические ошибки могут использоваться для обхода систем обнаружения плагиата~\cite{b8, b9}. Это приводит к возникновению состязательных атак на модели обнаружения плагиата~\cite{b10}, которые изначально обучались на наборах данных, содержащих минимальное количество ошибок. 

В настоящее время большинство методов автоматического исправления грамматики ориентированы на английский язык~\cite{b11}, где подобные системы показывают высокую эффективность. В то время как для других языков, включая русский, количество исследований остается ограниченным~\cite{b12}. Основная сложность заключается в морфологическом богатстве русского языка, что создает существенные препятствия для разработки эффективных систем коррекции грамматических ошибок.

% Среди наиболее результативных подходов к автоматическому исправлению грамматических ошибок для английского языка можно выделить два: Sequence-to-Sequence (Seq2Seq)~\cite{b13, b14} и Sequence Tagging (ST)~\cite{b15, b16}. В рамках Seq2Seq-методов модель получает на вход предложение с ошибками и генерирует правильный вариант. Хотя такие модели демонстрируют высокое качество исправлений, они обладают рядом недостатков: низкая скорость обработки и сложность интерпретации, так как для выявления типов ошибок требуется дополнительный анализ. Подход ST лишён этих недостатков: он не требует полной генерации исправленного текста, а лишь помечает и исправляет ошибки в исходной последовательности. К тому же, модели ST лучше интерпретируемы, так как каждая ошибка классифицируется и соотносится с конкретным правилом из словаря исправлений, что делает объяснение корректировок прозрачным.

Среди наиболее эффективных подходов к автоматическому исправлению грамматических ошибок в английском языке выделяются две основные архитектуры: Sequence-to-Sequence (Seq2Seq)~\cite{b13, b14} и Sequence Tagging (ST)~\cite{b15, b16}.

Модели типа Seq2Seq работают по принципу “чёрного ящика”: на вход подаётся ошибочное предложение, а на выходе генерируется исправленный вариант. Несмотря на высокую точность таких систем, они имеют существенные ограничения:

\begin{enumerate}
    \item низкая производительность из-за вычислительной сложности полной генерации текста;
    
    \item слабая интерпретируемость результатов, требующая дополнительного анализа для определения типов ошибок.
\end{enumerate}

В то время как при использовании моделей на основе архитектуры ST исправление происходит на уровне токенов, ошибки помечаются и корректируются в исходной последовательности без полной перегенерации текста. Также данное решение является более интепретируемым, поскольку каждое исправление соотносится с конкретным грамматическим правилом из словаря. Также с точки зрения эффективности обеспечивается лучшая производительность за счёт локального характера исправлений

В исследовании~\cite{b12} проводится сравнительный анализ данных подходов на материале русскоязычных текстов. Результаты показывают, что ST-модели значительно превосходят Seq2Seq при ограниченном объеме размеченных данных. Эксперименты выполнены на корпусе RULEC, который был разработан авторами исследования. Набор данных состоит из предложений иностранных студентов с аннотированными грамматическими ошибками. Данный корпус был использован для оценки работы нашей модели в экспериментах, но не был использован для ее обучения. 


GECToR~\cite{b15} в настоящее время считается одной из наиболее эффективных ST-моделей для английского языка. Модель построена на основе архитектуры трансформер~\cite{b17} и использует двухголовый механизм классификации. Первая голова отвечает за детекцию ошибок, вторая — за определение типа исправления. Если ошибка не обнаружена, применяется специальная метка “\$KEEP”, сохраняющая исходный токен без изменений. В экспериментах были рассмотрены различные кодировщики: BERT~\cite{b18}, RoBERTa~\cite{b19} и XLNet~\cite{b20}. Обучение модели проводилось в три этапа: обучение на синтетических данных, дообучение на корпусе с ошибками и дообучение на объединённом корпусе с ошибками и без них.

В рамках магистерской работы были выполнены два ключевых исследования:
\begin{enumerate}
    \item В рамках первого исследования был разработан алгоритм генерации синтетических данных для русского языка. На его основе создана русскоязычная адаптация модели GECToR — RuGECToR~\cite{b21}. Хотя обучение проводилось исключительно на синтетических данных, модель была ориентирована на выявление и обобщение грамматических закономерностей русского языка, а не на простое запоминание примеров.
    \item Предложен способ автоматического построения словаря корректирующих правил и последующего обучения ST-моделей с его использованием. Метод опирается на уровень SentencePiece-токенов~\cite{b22} и ограничивается универсальными преобразованиями: \textit{keep}, \textit{append}, \textit{replace} и \textit{delete}. Такой подход позволяет устранить зависимость от грамматических правил, полностью автоматизируя процесс. Метод был реализован на примере модели GECToR и показал сопоставимые результаты, при этом обеспечивая лёгкую адаптацию к другим языкам без необходимости ручной аннотации или знаний грамматики.
\end{enumerate}

