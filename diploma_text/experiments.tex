\newpage


\subsection{Описание набора данных}

\newcommand{\RomanNumeralCaps}[1]{\MakeUppercase{\romannumeral #1}}

\begin{table*}[ht!]
\fontsize{10pt}{12pt}\selectfont
\centering
\begin{tabular}{| p{4.5cm} | p{4.5cm} | p{6cm} |}
  \hline
  \textbf{исходное предложение}  & \textbf{предложение с ошибками} & \textbf{корректирующие правила} \\ 
  \hline
  \foreignlanguage{russian}{\textcolor{red}{'Человеческий', 'дух'}, 'непостижимо', 'могуч', ',', и', 'убить', 'его', 'в', \textcolor{red}{'человеке'}, 'почти', 'невозможно', '.'}
  & \foreignlanguage{russian}{\textcolor{red}{'Человеческий-дух'}, 'непостижимо', 'могуч', ',', 'и', 'убить', 'его', 'в', \textcolor{red}{'чел овеке'}, 'почти', 'невозможно', '.' }
  & {\textcolor{red}{\$TRANSFORM\_SPLIT\_HYPHEN}, \$KEEP, \$KEEP, \$KEEP,
  \$KEEP, \$KEEP, \$KEEP, \$KEEP, \textcolor{red}{\$MERGE\_SPACE}, \$KEEP, \$KEEP, \$KEEP, \$KEEP} \\ 
  \hline
\end{tabular}
\caption{Пример генерации ошибок в предложении}
\label{tab1}
\end{table*}

Для генерации синтетических ошибок на первом этапе обучения взяты предложения из русскоязычной Википедии и школьных сочинений~\cite{b24}. Для второго этапа обучения использованы школьные сочинения и литературные тексты~\cite{b25}. Сгенерированы специализированные ошибки для следующих частей речи: глагол, имя прилагательное, имя существительное, местоимение, причастие и числительное. Перед началом процесса генерации ошибок пронумерованы позиции токенов для указанных частей речи во всех предложениях.
Процесс генерации ошибок осуществляется следующим образом: 
\begin{enumerate}
    \item случайным образом выбирается предложение, где для каждого токена случайным образом генерируется ошибка, соответствующая части речи токена;
    \item после этого каждому такому токену ставится в соответствие правило из словаря корректирующих правил, которое необходимо применить для исправления ошибки.
\end{enumerate}
Распределение ошибок близко к равномерному. В табл.~\ref{tab1} показан пример генерации ошибок в предложении.


\begin{table}[ht!]
\begin{center}
\begin{tabular}{|l|c|c|c|}
  \hline
  \textbf{набор данных} & \textbf{\#предложений} & \makecell{\textbf{\% предложений} \\ \textbf{с ошибками}} & \makecell{\textbf{этап} \\ \textbf{обучения}} \\ \hline
  Wiki + Essays & $10\,000\,000$ & $\approx 100\%$ & \RomanNumeralCaps{1} \\ \hline
  Proza + Essays & $1\,000\,000$ & $\approx 50\%$ & \RomanNumeralCaps{2} \\ \hline
\end{tabular}
\caption{Информация об обучающем наборе данных}
\label{tab2}
\end{center}
\end{table}


В табл.~\ref{tab2} показано, что для первого этапа обучения использовалось 10.000.000 предложений, где каждое предложение содержит произвольные типы ошибок кроме пунктуационных. Для второго этапа обучения мы использовали 1.000.000 предложений: 500.000 предложений не содержат ошибок; 250.000 предложений содержат все ошибки, кроме пунктуационных; оставшиеся 250.000 предложений содержат только пунктуационные ошибки. Для проведения второго этапа обучения добавлены литературные произведения и исключена Википедия с целью сделать модель более устойчивой к различиям между наборами данных~\cite{b26}. В качестве тестовых наборов данных использовались школьные сочинения и тестовое подмножество набора данных RULEC. 
В работе~\cite{b15} авторы разделяют правила, применяемые к исходным токенам $\{x_1,x_2,…,x_{n_i} \}$ для получения целевого предложения, на два типа – базовые и грамматические. В нашем исследовании сохранено данное разделение. Грамматические правила подобраны таким образом, чтобы словарь корректирующих правил покрывал множество правил русского языка. Базовые правила выполняют наиболее распространенные операции редактирования на уровне токенов: сохранение текущего токена $x_i$ без изменений – правило $\$KEEP$, удаление текущего токена $x_i$ – правило $\$DELETE$, добавление нового токена $t_1$ после текущего токена $x_i$ – правило $\$APPEND\_t_1$ или замена текущего токена $x_i$ на другой токен $t_2$ – правило $\$REPLACE\_t_2$. Для генерации правил \$APPEND и \$REPLACE использованы 2500 наиболее употребляемых русских слов с точки зрения коэффициента Жуайна~\cite{b27}. Для каждого слова $w_i$ добавлены соответствующие правила $\$APPEND\_w_i$ и $\$REPLACE\_w_i$.
Грамматические правила выполняют операции, специфичные для конкретного случая. Для русского языка использованы следующие правила, которые применяются непосредственно к токену: изменение времени, падежа, рода, лица и числа. Добавлены правила для часто допускаемых ошибок, например, правописание “ться”/“тся” и “при”/“пре”. Также использованы правила, которые не зависят от языка: объединение двух слов с помощью пробела или дефиса; разделение слова, написанного через дефис, на две части; изменение регистра первой буквы слова.
Рассмотрим пример корректирующего правила для исправления “красивая” на “красивый”. Таким образом, необходимо преобразовать прилагательное из женского рода в мужской. Изначально правила для грамматических преобразований создавались следующим образом:
\begin{center}
\$TRANSFORM\_{\color{orange} ADJF}\_GEND\_{\color{orange} femn}\_masc
\end{center}
Такие правила содержали подробную информацию о:
\begin{enumerate}
    \item части речи слова; 
    \item грамматическом признаке, который необходимо изменить; 
    \item граммеме как для исходного, так и для исправленного слова.
\end{enumerate}
 Далее было решено объединить правила для разных частей речи, исключив название части речи. Кроме того, решено отказаться от указания граммемы для исходного (неправильного) слова, поскольку для его исправления необходима информация только о новой граммеме. В данном исследовании предполагается, что модель сама научится сопоставлять правила соответственно частям речи. Например, глагол и прилагательное могут менять род, а существительное – нет. Таким образом, итоговые правила, используемые в нашей модели, выглядят следующим образом:
 \begin{center}
\$TRANSFORM\_GEND\_masc
\end{center}
Данная модификация была сделана для того, чтобы уменьшить размер словаря и увеличить количество примеров для каждого правила в обучающем наборе данных. Таким образом достигается компромисс между обобщающей способностью и размером модели, так как с ростом количества правил увеличивается размер слоя классификации. 

Для оценки полноты составленного набора правил было проведено сравнение с классификатором ошибок в наборе данных RULEC. Для большинства ошибок из набора данных RULEC в нашем словаре существуют аналогичные правила. Но в наборе данных RULEC существуют и более узкие правила, например, “Местоимение’’, “Сущ.: число”, означающие наличие ошибки в написании множественного/ единственного в соответствующих частях речи. Для данных ошибок в нашем словаре нет специализированных правил, но тем не менее они покрываются более широкими правилами для исправления написания множественного/единственного числа без привязывания к конкретным частям речи. Также существуют правила, такие как “Орфография”, “Союз” и “Предлог”, аналогов которым нет в нашем словаре. Исправление ошибок, связанных с данными правилами, предусмотрено правилами $\$APPEND\_w_i$ и $\$REPLACE\_w_i$.

\subsection{Эксперименты}
В качестве предобученного кодировщика на основе архитектуры трансформер выбран Multilingual BERT~\cite{b28}, имеющий хорошую языковую обобщаю способность. Эксперименты по сравнению различных кодировщиков на основе архитектуры трансформер запланированы в дальнейших работах. Обучение модели проводилось в течение двух этапов, каждый из которых длился 50 эпох. На первом этапе обучения размер батча равен 32, на втором – 16. В качестве оптимизатора параметров выбран Adam с параметром $lr=10^{-5}$. 
Для работы модели на этапе применения было выявлено, что количество исправлений уменьшается с каждой успешной итерацией, и необходимое число исправлений выполняется в течение первых трех итераций. 

\subsection{Метрики качества}

Сравнение качества работы моделей проводилось на синтетических и реальных данных. Для оценки качества использованы метрики, описанные в\cite{b11}:

\begin{equation*}
    R = \frac{\sum_{i=1}^{N} |g_i \cap e_i|}{\sum_{i=1}^{N} |g_i|}, \quad
    P = \frac{\sum_{i=1}^{N} |g_i \cap e_i|}{\sum_{i=1}^{N} |e_i|},
\end{equation*}
\begin{equation*}
    F_{0.5} = \frac{(1 + 0.5^2) \cdot R \cdot P}{R + 0.5^2 \cdot P},
\end{equation*}

где $N$~-- количество предложений, $e_i$~-- множество исправлений, предсказанных нашей моделью для предложения $s_i$, а $g_i$~-- множество эталонных исправлений. Пересечение между $g_i$ и $e_i$ определяется как:

\begin{equation*}
    g_i \cap e_i = \{ e \in e_i \mid \exists g \in g_i \colon e = g \}.
\end{equation*}


